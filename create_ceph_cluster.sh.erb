#!/bin/bash
#Wes Dillingham
#wes_dillingham@harvard.edu
#Research Computing - Harvard University 
#All Rights Reserved



###### DO NOT Modify this script outside of the puppet repo, it is a template generated by puppet.
###### Any modifications directly to this file will be overwritten @ the next Puppet Run
#For every Ceph cluster, a group of scripts is to be deployed, including this one, it is dynamically populated by 
#Puppet, as a template, and is specific only to the cluster it is generated for, this is not a general pupose 
#script for use with ANY given ceph cluster, it is only to be used with this cluster: <$CLUSTERNAME>


#########   PURPOSE: ##########
#This scripts goal is to build a fully functional (Active+Clean) Ceph Cluster from bare hardware.
#It builds out ceph MDS, Ceph Mon and Ceph OSD nodes, and activates the components.

##### REQUIREMENTS ###########
#This script needs the variables set in the included "source.sh" file. Source them and then run this script. 
#This script expects that the target systems will have an OS installed, listening ssh server, 
# On the OSD hosts that the disks will be raw and clean of any previous configuration.

#Functions:

function getPowTwo() {
#this function find the first power of 2 greater than the number passed to it 
let i=2
while [[ k -lt $1 ]]
do
  k=$((2**$i))
  i=$((i+1))
done
PGNUM=$k
}

SCRIPTLOC=`readlink -f $0 | rev | cut -d "/" -f2- | rev`

#Gather Hostkeys
<% for @mon in @mon_hosts -%>
ssh-keyscan -H <%= @mon %> >> ~/.ssh/known_hosts
<% end -%>
<% for @mds in @mds_hosts -%>
ssh-keyscan -H <%= @mds %> >> ~/.ssh/known_hosts
<% end -%>
<% for @osd in @osd_hosts -%>
ssh-keyscan -H <%= @osd %> >> ~/.ssh/known_hosts
<% end -%>

#Prepare Firewall
<% for @mon in @mon_hosts -%>
ssh -t <%= @mon %> "sudo service firewalld stop"
<% end -%>
<% for @mds in @mds_hosts -%>
ssh -t <%= @mds %> "sudo service firewalld stop"
<% end -%>
<% for @osd in @osd_hosts -%>
ssh -t <%= @mon %> "sudo service firewalld stop"
<% end -%>


#disable requiretty
<% for @mon in @mon_hosts -%>
ssh -t <%= @mon %> "sudo sed -i 's/^Defaults    requiretty/Defaults:ceph !requiretty/' /etc/sudoers"
<% end -%>
<% for @mds in @mds_hosts -%>
ssh -t <%= @mds %> "sudo sed -i 's/^Defaults    requiretty/Defaults:ceph !requiretty/' /etc/sudoers"
<% end -%>
<% for @osd in @osd_hosts -%>
ssh -t <%= @osd %> "sudo sed -i 's/^Defaults    requiretty/Defaults:ceph !requiretty/' /etc/sudoers"
<% end -%>
sudo sed -i 's/^Defaults    requiretty/Defaults:ceph !requiretty/' /etc/sudoers


#####Mons

#Deploy
#ceph-deploy --cluster $_CLUSTER_NAME new $_MON0 $_MON1 $_MON2 #custom cluster naming not supported on sysvinit hosts yet
ceph-deploy new <%= @mon_hosts.join(" ") -%> #check me
#A ceph.conf a ceph.log and a mon keyring are now created withing the current working directory
echo "osd pool default size = <%= @name %>" >> $SCRIPTLOC/ceph.conf

#need to determine where the root disk is, either /dev/vda1 or /dev/sda1 this is important because if root disk is at /dev/sda1 we should not attempt use it for ceph. We do this by adding in a grep -v when zapping, deploying, and activating
<% for @osd in @osd_hosts -%>
DEVPREFIX_<%= @osd.tr('-','_').split(".")[0] %>=$(ssh -q -t <%= @osd %> df -h / | grep -i dev | awk -F " " '{print $1}'| awk -F "/" '{print $3}' | cut -c1,2,3 )
<% end -%>

#Need to determine The number of OSDs in the entire cluster, this is important for calculating the number of PGS
#PGs are determined using the following guideline from here: http://ceph.com/docs/master/rados/operations/placement-groups/
#Less than 5 OSDs set pg_num to 128
#Between 5 and 10 OSDs set pg_num to 512
#Between 10 and 50 OSDs set pg_num to 4096
#If you have more than 50 OSDs, Calculate using the following equation:
#Total PGs = (OSDs * 100) / pool size
#Where pool size is either the # of replicas for replicated pools or the K+M sum for erasure coded pools (as returned by ceph osd erasure code-profile get)

<% for @osd in @osd_hosts -%>
for i in `ssh <%= @osd.split(".")[0] %> "lsblk --output KNAME | grep -i sd | grep -v vda | grep -v [0-9]"`; do OSDCOUNT_<%= @osd.tr('-','_').split(".")[0] %>=$((OSDCOUNT_<%= @osd.tr('-','_').split(".")[0] %>+1)); done
<% end -%>

<% for @osd in @osd_hosts -%>
TOTALOSDCOUNT=$((TOTALOSDCOUNT + OSDCOUNT_<%= @osd.split(".")[0] %>))
<% end -%>

echo "Number of OSDs: $TOTALOSDCOUNT"
echo "Number of Replicas: <%= @numreplicas %>"

#implement logic to set number of placement groups  appropriately
if [ $TOTALOSDCOUNT -le 5 ];
then
        PGNUM=128
elif [[ $TOTALOSDCOUNT -gt 5 && $TOTALOSDCOUNT -le 10 ]];
then
        PGNUM=512
elif [[ $TOTALOSDCOUNT -gt 10 && $TOTALOSDCOUNT -le 61 ]];
then
        PGNUM=4096
elif [ $TOTALOSDCOUNT -gt 61 ];
then
        TEMP_PGNUM=$((TOTALOSDCOUNT * 100  / <%= @numreplicas %>)) #check me
        getPowTwo "$TEMP_PGNUM"
else
        echo "ERROR variable:TOTALOSDCOUNT (total number of osds in this cluster) is not a usable number, cannot determine # of PGs based on its value: $TOTALOSDCOUNT"
fi

echo "PGNUM is $PGNUM"

#need to modify ceph.conf to use calculated pg and pgp
echo "osd pool default pg num = $PGNUM" >> $SCRIPTLOC/ceph.conf
echo "osd pool default pgp num = $PGNUM" >> $SCRIPTLOC/ceph.conf

#Install ceph
ceph-deploy install <%= @mon_hosts.join(' ') -%> <%= @mds_hosts.join(' ') -%> <%= @osd_hosts.join(' ') -%> #check me

#Copy Mon Keyring
<% for @osd in @osd_hosts -%>
rsync -avhP --rsync-path="sudo rsync" ceph.mon.keyring <%= @osd %>:/etc/ceph/ceph.mon.keyring
<% end -%>

# wait until they form quorum and then
# gatherkeys, reporting the monitor status along the
# process. If monitors don't form quorum the command
# will eventually time out.
ceph-deploy mon create-initial

#gather keys
ceph-deploy gatherkeys <%= @mon_hosts[0] %> #check me

#####OSDs


#first zap the disks
<% for @osd in @osd_hosts -%>
for i in `ssh <%= @osd %> "lsblk --output KNAME | grep -i sd | grep -v $DEVPREFIX_<%= @osd.tr('-','_').split(".")[0] %> | grep -v [0-9]"`; do ceph-deploy disk zap <%= @osd %>:$i; done
<% end -%>

#Deploy the OSDs
<% for @osd in @osd_hosts -%>
for i in `ssh <%= @osd %> "lsblk --output KNAME | grep -i sd | grep -v $DEVPREFIX_<%= @osd.tr('-','_').split(".")[0] %>"`; do ceph-deploy osd prepare <%= @osd %>:$i:$i; done
<% end -%>

#Activate the OSDs
<% for @osd in @osd_hosts -%>
for i in `ssh <%= @osd %> "lsblk --output KNAME | grep -i sd | grep -v $DEVPREFIX_<%= @osd.tr('-','_').split(".")[0] %> | grep 1"`; do ceph-deploy osd activate <%= @osd %>:${i}; done
<% end -%>
#At this point the ceph-deploy is complete, 1 pool exists called "rbd" and it doesnt appear to be obeying the rules of placement groups defined in our ceph.conf


#create pool as defined by _INITIAL_POOL in source.sh and set it to use the calculated amount of placement groups
for pool in <%= @pools.join(" ") %>
do
	ssh -t <%= @mon_hosts[0] %> "sudo ceph osd pool create $pool $PGNUM"
	echo "going to sleep for 20 seconds to get the cluster time to create the placement groups for pool $pool"
	sleep 20 #give pgs time to be created, this may need to be adjusted, or logic needs to be built in to wait till pg's are built
	ssh -t <%= @mon_hosts[0] %> "sudo ceph osd pool set $pool pgp_num $PGNUM" #this will cause rebalancing, but the cluster is empty, of course
done
