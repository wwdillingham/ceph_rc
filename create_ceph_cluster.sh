#/bin/bash
#Wes Dillingham
#wes_dillingham@harvard.edu
#Research Computing - Harvard University 
#All Rights Reserved

###### DO NOT Modify this script outside of the puppet repo, it is a template generated by puppet.
###### Any modifications directly to this file will be overwritten @ the next Puppet Run
#For every Ceph cluster, a group of scripts is to be deployed, including this one, it is dynamically populated by 
#Puppet, as a template, and is specific only to the cluster it is generated for, this is not a general pupose 
#script for use with ANY given ceph cluster, it is only to be used with this cluster: <$CLUSTERNAME>


#########   PURPOSE: ##########
#This scripts goal is to build a fully functional (Active+Clean) Ceph Cluster from bare hardware.
#It builds out ceph MDS, Ceph Mon and Ceph OSD nodes, and activates the components.
#This script expects that the target systems will have an OS installed, a user (cephrc) with passwordless sudo, 
# On the OSD hosts that the disks will be raw and clean of any previous configuration.

#Functions:
function getPowTwo() {
#this function find the first power of 2 greater than the number passed to it 
let i=2
while [[ k -lt $1 ]]
do
  k=$((2**$i))
  i=$((i+1))
done
PGNUM=$k
}

#Make This Directory owned by CEPH_ADMIN_USER
sudo chown -R $CEPH_ADMIN_USER:$CEPH_ADMIN_USER /tmp/userdata_launchpad


#Gather Hostkeys
ssh-keyscan -H $_MON0 >> ~/.ssh/known_hosts
ssh-keyscan -H $_MON1 >> ~/.ssh/known_hosts
ssh-keyscan -H $_MON2 >> ~/.ssh/known_hosts
ssh-keyscan -H $_MDS0 >> ~/.ssh/known_hosts
ssh-keyscan -H $_OSD0 >> ~/.ssh/known_hosts
ssh-keyscan -H $_OSD1 >> ~/.ssh/known_hosts
ssh-keyscan -H $_OSD2 >> ~/.ssh/known_hosts

#Prepare Firewall
ssh -t $_MON0 "sudo service firewalld stop"
ssh -t $_MON1 "sudo service firewalld stop"
ssh -t $_MON2 "sudo service firewalld stop"
ssh -t $_MDS0 "sudo service firewalld stop"
ssh -t $_OSD0 "sudo service firewalld stop"
ssh -t $_OSD1 "sudo service firewalld stop"
ssh -t $_OSD2 "sudo service firewalld stop"
sudo service firewalld stop

#disable requiretty
ssh -t $_MON0 "sudo sed -i 's/^Defaults    requiretty/Defaults:ceph !requiretty/' /etc/sudoers"
ssh -t $_MON1 "sudo sed -i 's/^Defaults    requiretty/Defaults:ceph !requiretty/' /etc/sudoers"
ssh -t $_MON2 "sudo sed -i 's/^Defaults    requiretty/Defaults:ceph !requiretty/' /etc/sudoers"
ssh -t $_MDS0 "sudo sed -i 's/^Defaults    requiretty/Defaults:ceph !requiretty/' /etc/sudoers"
ssh -t $_OSD0 "sudo sed -i 's/^Defaults    requiretty/Defaults:ceph !requiretty/' /etc/sudoers"
ssh -t $_OSD1 "sudo sed -i 's/^Defaults    requiretty/Defaults:ceph !requiretty/' /etc/sudoers"
ssh -t $_OSD2 "sudo sed -i 's/^Defaults    requiretty/Defaults:ceph !requiretty/' /etc/sudoers"
sudo sed -i 's/^Defaults    requiretty/Defaults:ceph !requiretty/' /etc/sudoers

#Start NTP
rsync --rsync-path="sudo rsync" ntp.conf $_MON0:/etc/ntp.conf
rsync --rsync-path="sudo rsync" ntp.conf $_MON1:/etc/ntp.conf
rsync --rsync-path="sudo rsync" ntp.conf $_MON2:/etc/ntp.conf
rsync --rsync-path="sudo rsync" ntp.conf $_MDS0:/etc/ntp.conf
rsync --rsync-path="sudo rsync" ntp.conf $_OSD0:/etc/ntp.conf
rsync --rsync-path="sudo rsync" ntp.conf $_OSD1:/etc/ntp.conf
rsync --rsync-path="sudo rsync" ntp.conf $_OSD2:/etc/ntp.conf
ssh -t $_MON0 "sudo systemctl start ntpd && sudo systemctl enable ntpd"
ssh -t $_MON1 "sudo systemctl start ntpd && sudo systemctl enable ntpd"
ssh -t $_MON2 "sudo systemctl start ntpd && sudo systemctl enable ntpd"
ssh -t $_MDS0 "sudo systemctl start ntpd && sudo systemctl enable ntpd"
ssh -t $_OSD0 "sudo systemctl start ntpd && sudo systemctl enable ntpd"
ssh -t $_OSD1 "sudo systemctl start ntpd && sudo systemctl enable ntpd"
ssh -t $_OSD2 "sudo systemctl start ntpd && sudo systemctl enable ntpd"
sudo systemctl start ntpd && sudo systemctl enable ntpd


#####Mons

#Deploy
ceph-deploy new $_MON0 $_MON1 $_MON2 #initial monitor members

#Install ceph
ceph-deploy install $_MON0 $_MON1 $_MON2 $_MDS0 $_OSD0 $_OSD1 $_OSD2
#At this point a ceph.conf has been generated in the current working directory of the admin script.

#Copy Mon Keyring
rsync -avhP --rsync-path="sudo rsync" ceph.mon.keyring $_OSD0:/etc/ceph/ceph.mon.keyring
rsync -avhP --rsync-path="sudo rsync" ceph.mon.keyring $_OSD1:/etc/ceph/ceph.mon.keyring
rsync -avhP --rsync-path="sudo rsync" ceph.mon.keyring $_OSD2:/etc/ceph/ceph.mon.keyring

# wait until they form quorum and then
# gatherkeys, reporting the monitor status along the
# process. If monitors don't form quorum the command
# will eventually time out.
ceph-deploy mon create-initial

#gather keys
ceph-deploy gatherkeys $_MON0

#####OSDs

#need to determine where the root disk is, either /dev/vda1 or /dev/sda1 this is important because if root disk is at /dev/sda1 we should not attempt use it for ceph. We do this by adding in a grep -v when zapping, deploying, and activating
DEVPREFIX_OSD0=$(ssh -q -t $_OSD0 df -h / | grep -i dev | awk -F " " '{print $1}'| awk -F "/" '{print $3}' | cut -c1,2,3 )
DEVPREFIX_OSD1=$(ssh -q -t $_OSD0 df -h / | grep -i dev | awk -F " " '{print $1}'| awk -F "/" '{print $3}' | cut -c1,2,3 )
DEVPREFIX_OSD2=$(ssh -q -t $_OSD0 df -h / | grep -i dev | awk -F " " '{print $1}'| awk -F "/" '{print $3}' | cut -c1,2,3 )

#Need to determine The number of OSDs in the entire cluster, this is important for calculating the number of PGS
#PGs are determined using the following guideline from here: http://ceph.com/docs/master/rados/operations/placement-groups/
#Less than 5 OSDs set pg_num to 128
#Between 5 and 10 OSDs set pg_num to 512
#Between 10 and 50 OSDs set pg_num to 4096
#If you have more than 50 OSDs, Calculate using the following equation:
#Total PGs = (OSDs * 100) / pool size
#Where pool size is either the # of replicas for replicated pools or the K+M sum for erasure coded pools (as returned by ceph osd erasure code-profile get)

for i in `ssh $_OSD0 "lsblk --output KNAME | grep -i sd | grep -v vda | grep -v [0-9]"`; do OSDCOUNT_OSD0=$((OSDCOUNT_OSD0+1)); done #line needs to be generates per OSD host
for i in `ssh $_OSD0 "lsblk --output KNAME | grep -i sd | grep -v vda | grep -v [0-9]"`; do OSDCOUNT_OSD1=$((OSDCOUNT_OSD1+1)); done #line needs to be generates per OSD host
for i in `ssh $_OSD0 "lsblk --output KNAME | grep -i sd | grep -v vda | grep -v [0-9]"`; do OSDCOUNT_OSD2=$((OSDCOUNT_OSD2+1)); done #line needs to be generates per OSD host
TOTALOSDCOUNT= $OSDCOUNT_OSD0 + $OSDCOUNT_OSD1 + $OSDCOUNT_OSD2 #This summation list is dynamically generated in the puppet template

echo "Number of OSDs: $TOTALOSDCOUNT"
echo "Number of Replicas: $_NUM_REPLICAS"

#implement logic to set number of placement groups  appropriately
if [ $TOTALOSDCOUNT -le 5 ];
then
        PGNUM=128
elif [[ $TOTALOSDCOUNT -gt 5 && $TOTALOSDCOUNT -le 10 ]];
then
        PGNUM=512
elif [[ $TOTALOSDCOUNT -gt 10 && $TOTALOSDCOUNT -le 61 ]];
then
        PGNUM=4096
elif [ $TOTALOSDCOUNT -gt 61 ];
then
        TEMP_PGNUM=$((TOTALOSDCOUNT * 100  / _NUM_REPLICAS))
        getPowTwo "$TEMP_PGNUM"
else
        echo "ERROR variable:TOTALOSDCOUNT (total number of osds in this cluster) is not a usable number, cannot determine # of PGs based on its value: $TOTALOSDCOUNT"
fi

echo "PGNUM is $PGNUM"

#Modify the conf to reflect the new number of placement groups


#first zap the disks
for i in `ssh $_OSD0 "lsblk --output KNAME | grep -i sd | grep -v $DEVPREFIX_OSD0 | grep -v [0-9]"`; do ceph-deploy disk zap $_OSD0:$i; done
for i in `ssh $_OSD1 "lsblk --output KNAME | grep -i sd | grep -v $DEVPREFIX_OSD1 | grep -v [0-9]"`; do ceph-deploy disk zap $_OSD1:$i; done
for i in `ssh $_OSD2 "lsblk --output KNAME | grep -i sd | grep -v $DEVPREFIX_OSD2 | grep -v [0-9]"`; do ceph-deploy disk zap $_OSD2:$i; done

#Deploy the OSDs
for i in `ssh $_OSD0 "lsblk --output KNAME | grep -i sd | grep -v $DEVPREFIX_OSD0"`; do ceph-deploy osd prepare $_OSD0:$i:$i; done
for i in `ssh $_OSD1 "lsblk --output KNAME | grep -i sd | grep -v $DEVPREFIX_OSD1"`; do ceph-deploy osd prepare $_OSD1:$i:$i; done
for i in `ssh $_OSD2 "lsblk --output KNAME | grep -i sd | grep -v $DEVPREFIX_OSD2"`; do ceph-deploy osd prepare $_OSD2:$i:$i; done

#Activate the OSDs
for i in `ssh $_OSD0 "lsblk --output KNAME | grep -i sd | grep -v $DEVPREFIX_OSD0 | grep 1"`; do ceph-deploy osd activate $_OSD0:${i}; done
for i in `ssh $_OSD1 "lsblk --output KNAME | grep -i sd | grep -v $DEVPREFIX_OSD1 | grep 1"`; do ceph-deploy osd activate $_OSD1:${i}; done
for i in `ssh $_OSD2 "lsblk --output KNAME | grep -i sd | grep -v $DEVPREFIX_OSD2 | grep 1"`; do ceph-deploy osd activate $_OSD2:${i}; done

